{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "train = pd.read_csv(\n",
    "    r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\train.csv')\n",
    "test = pd.read_csv(\n",
    "    r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\test.csv')\n",
    "full_data = [train, test]\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pclass ##\n",
    "\n",
    "票类：经济地位的象征\n",
    "\n",
    "序号 | 票类\n",
    "---- | ----\n",
    "1 | 头等舱\n",
    "2 | 中等舱\n",
    "3 | 末等舱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name   Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris  male  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1  \n"
     ]
    }
   ],
   "source": [
    "# One-hot编码\n",
    "# train\n",
    "train['P1'] = np.array(train['Pclass'] == 1).astype(np.int32)\n",
    "train['P2'] = np.array(train['Pclass'] == 2).astype(np.int32)\n",
    "train['P3'] = np.array(train['Pclass'] == 3).astype(np.int32)\n",
    "# test\n",
    "test['P1'] = np.array(test['Pclass'] == 1).astype(np.int32)\n",
    "test['P2'] = np.array(test['Pclass'] == 2).astype(np.int32)\n",
    "test['P3'] = np.array(test['Pclass'] == 3).astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sex ##\n",
    "\n",
    "性别：男or女\n",
    "\n",
    "Sex | label\n",
    "---- | ----\n",
    "male | 1\n",
    "female | 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1  \n"
     ]
    }
   ],
   "source": [
    "# 把male/female转换成1/0\n",
    "train['Sex'] = [1 if i == 'male' else 0 for i in train.Sex]\n",
    "test['Sex'] = [1 if i == 'male' else 0 for i in test.Sex]\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SibSp and Parch ##\n",
    "\n",
    "- SibSp\n",
    "\n",
    "the number of siblings/spouse：兄弟姐妹/配偶人数\n",
    "\n",
    "- Parch\n",
    "\n",
    "the number of children/parents：子女/父母人数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2  \n"
     ]
    }
   ],
   "source": [
    "# 'FamilySize'：家庭成员人数\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  IsAlone  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2        0  \n"
     ]
    }
   ],
   "source": [
    "# 'IsAlone'：是否只身一人\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embarked ##\n",
    "\n",
    "登船港口，有缺失值，先进行缺失值处理\n",
    "\n",
    "C = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  IsAlone  E1  \\\n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2        0   1   \n",
      "\n",
      "   E2  E3  \n",
      "0   0   0  \n"
     ]
    }
   ],
   "source": [
    "# 缺失值处理\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# One-hot编码\n",
    "# train\n",
    "train['E1'] = np.array(train['Embarked'] == 'S').astype(np.int32)\n",
    "train['E2'] = np.array(train['Embarked'] == 'C').astype(np.int32)\n",
    "train['E3'] = np.array(train['Embarked'] == 'Q').astype(np.int32)\n",
    "# test\n",
    "test['E1'] = np.array(test['Embarked'] == 'S').astype(np.int32)\n",
    "test['E2'] = np.array(test['Embarked'] == 'C').astype(np.int32)\n",
    "test['E3'] = np.array(test['Embarked'] == 'Q').astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fare ##\n",
    "\n",
    "乘客票价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... FamilySize IsAlone  E1  E2  E3  CategoricalFare  \\\n",
      "0      0  A/5 21171  7.25 ...          2       0   1   0   0                1   \n",
      "\n",
      "   F1  F2  F3  F4  \n",
      "0   1   0   0   0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "train['CategoricalFare'].cat.categories = [1, 2, 3, 4]\n",
    "# one-hot编码\n",
    "train['F1'] = np.array(train['CategoricalFare'] == 1).astype(np.int32)\n",
    "train['F2'] = np.array(train['CategoricalFare'] == 2).astype(np.int32)\n",
    "train['F3'] = np.array(train['CategoricalFare'] == 3).astype(np.int32)\n",
    "train['F4'] = np.array(train['CategoricalFare'] == 4).astype(np.int32)\n",
    "\n",
    "# test\n",
    "test['CategoricalFare'] = pd.qcut(test['Fare'], 4)\n",
    "test['CategoricalFare'].cat.categories = [1, 2, 3, 4]\n",
    "# one-hot编码\n",
    "test['F1'] = np.array(test['CategoricalFare'] == 1).astype(np.int32)\n",
    "test['F2'] = np.array(test['CategoricalFare'] == 2).astype(np.int32)\n",
    "test['F3'] = np.array(test['CategoricalFare'] == 3).astype(np.int32)\n",
    "test['F4'] = np.array(test['CategoricalFare'] == 4).astype(np.int32)\n",
    "\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Age ##\n",
    "\n",
    "缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... FamilySize IsAlone  E1  E2  E3  CategoricalFare  \\\n",
      "0      0  A/5 21171  7.25 ...          2       0   1   0   0                1   \n",
      "\n",
      "   F1  F2  F3  F4  \n",
      "0   1   0   0   0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(\n",
    "        age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... F1 F2  F3  F4  CategoricalAge  A1  A2  A3  A4  \\\n",
      "0      0  A/5 21171  7.25 ...  1  0   0   0               2   0   1   0   0   \n",
      "\n",
      "   A5  \n",
      "0   0  \n",
      "\n",
      "[1 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['CategoricalAge'] = pd.qcut(train['Age'], 5)\n",
    "train['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]\n",
    "train['A1'] = np.array(train['CategoricalAge'] == 1).astype(np.int32)\n",
    "train['A2'] = np.array(train['CategoricalAge'] == 2).astype(np.int32)\n",
    "train['A3'] = np.array(train['CategoricalAge'] == 3).astype(np.int32)\n",
    "train['A4'] = np.array(train['CategoricalAge'] == 4).astype(np.int32)\n",
    "train['A5'] = np.array(train['CategoricalAge'] == 5).astype(np.int32)\n",
    "# test\n",
    "test['CategoricalAge'] = pd.qcut(test['Age'], 5)\n",
    "test['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]\n",
    "test['A1'] = np.array(test['CategoricalAge'] == 1).astype(np.int32)\n",
    "test['A2'] = np.array(test['CategoricalAge'] == 2).astype(np.int32)\n",
    "test['A3'] = np.array(test['CategoricalAge'] == 3).astype(np.int32)\n",
    "test['A4'] = np.array(test['CategoricalAge'] == 4).astype(np.int32)\n",
    "test['A5'] = np.array(test['CategoricalAge'] == 5).astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Name ##\n",
    "\n",
    "新增一列特征'Title'：头衔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex         0    1\n",
      "Title             \n",
      "Capt        0    1\n",
      "Col         0    2\n",
      "Countess    1    0\n",
      "Don         0    1\n",
      "Dr          1    6\n",
      "Jonkheer    0    1\n",
      "Lady        1    0\n",
      "Major       0    2\n",
      "Master      0   40\n",
      "Miss      182    0\n",
      "Mlle        2    0\n",
      "Mme         1    0\n",
      "Mr          0  517\n",
      "Mrs       125    0\n",
      "Ms          1    0\n",
      "Rev         0    6\n",
      "Sir         0    1\n"
     ]
    }
   ],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "  'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "print(train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... A2 A3  A4  A5  Title  T1  T2  T3  T4  T5  \n",
      "0      0  A/5 21171  7.25 ...  1  0   0   0     Mr   0   0   1   0   0  \n",
      "\n",
      "[1 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['T1'] = np.array(train['Title'] == 'Master').astype(np.int32)\n",
    "train['T2'] = np.array(train['Title'] == 'Miss').astype(np.int32)\n",
    "train['T3'] = np.array(train['Title'] == 'Mr').astype(np.int32)\n",
    "train['T4'] = np.array(train['Title'] == 'Mrs').astype(np.int32)\n",
    "train['T5'] = np.array(train['Title'] == 'Rare').astype(np.int32)\n",
    "# test\n",
    "test['T1'] = np.array(test['Title'] == 'Master').astype(np.int32)\n",
    "test['T2'] = np.array(test['Title'] == 'Miss').astype(np.int32)\n",
    "test['T3'] = np.array(test['Title'] == 'Mr').astype(np.int32)\n",
    "test['T4'] = np.array(test['Title'] == 'Mrs').astype(np.int32)\n",
    "test['T5'] = np.array(test['Title'] == 'Rare').astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗 #\n",
    "\n",
    "获得训练神经网络的数据：**train_x，train_y_**\n",
    "\n",
    "以及预测样本：**test_x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'P1', 'P2', 'P3',\n",
       "       'FamilySize', 'IsAlone', 'E1', 'E2', 'E3', 'CategoricalFare', 'F1',\n",
       "       'F2', 'F3', 'F4', 'CategoricalAge', 'A1', 'A2', 'A3', 'A4', 'A5',\n",
       "       'Title', 'T1', 'T2', 'T3', 'T4', 'T5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   P1  P2  P3  Sex  IsAlone  E1  E2  E3  F1  F2 ...  A1  A2  A3  A4  A5  T1  \\\n",
      "0   0   0   1    1        0   1   0   0   1   0 ...   0   1   0   0   0   0   \n",
      "\n",
      "   T2  T3  T4  T5  \n",
      "0   0   1   0   0  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "train_x = train[[\n",
    "    'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3',\n",
    "    'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5'\n",
    "]]\n",
    "print(train_x.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived\n",
      "0         0\n"
     ]
    }
   ],
   "source": [
    "train_y_ = train[['Survived']]\n",
    "print(train_y_.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   P1  P2  P3  Sex  IsAlone  E1  E2  E3  F1  F2 ...  A1  A2  A3  A4  A5  T1  \\\n",
      "0   0   0   1    1        1   0   0   1   1   0 ...   0   0   0   1   0   0   \n",
      "\n",
      "   T2  T3  T4  T5  \n",
      "0   0   1   0   0  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "test_x = test[[\n",
    "    'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3',\n",
    "    'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5'\n",
    "]]\n",
    "print(test_x.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建神经网络 #\n",
    "\n",
    "## 神经网络V1 ##\n",
    "\n",
    "- 无隐藏层\n",
    "- 交叉熵损失函数\n",
    "- 指数衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "STEPS = 25000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.005\n",
    "DECAY_STEPS = 222750\n",
    "DECAY_RATE = 0.96\n",
    "\n",
    "\n",
    "def get_weight(shape):\n",
    "    w = tf.Variable(tf.random_normal(shape))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.random_normal(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    w = get_weight([22, 1])\n",
    "    b = get_bias([1])\n",
    "    y = tf.matmul(x, w) + b\n",
    "    pred = tf.cast(tf.sigmoid(y) > 0.5, tf.float32)\n",
    "    return y, pred\n",
    "\n",
    "\n",
    "def backward(train_x, train_y_, test_x):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 22])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    y, pred = forward(x)\n",
    "\n",
    "    # 定义损失函数loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decayed_learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE)\n",
    "\n",
    "    # 定义反向传播方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        decayed_learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            index = np.random.permutation(len(train_y_))\n",
    "            train_x = train_x.take(index)\n",
    "            train_y_ = train_y_.take(index)\n",
    "            for j in range(len(train_y_) // 100 + 1):\n",
    "                start = j * BATCH_SIZE\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "            if i % 1000 == 0:\n",
    "                train_loss_temp = sess.run(\n",
    "                    loss,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_loss.append(train_loss_temp)\n",
    "                train_acc_temp = sess.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_acc.append(train_acc_temp)\n",
    "                print(train_loss_temp, ' ', train_acc_temp)\n",
    "                print('global_step:',\n",
    "                      sess.run(global_step), 'decayed_learning_rate:',\n",
    "                      sess.run(decayed_learning_rate))\n",
    "        # 使用训练好的模型，喂入测试集数据\n",
    "        result = sess.run(pred, feed_dict={x: test_x})\n",
    "        # print('----------------')\n",
    "        # print('Prediction:')\n",
    "        # print(result)\n",
    "        submit = test[['PassengerId']]\n",
    "        submit.insert(1, 'Survived', result)\n",
    "        submit['Survived'] = submit['Survived'].astype(np.int32)\n",
    "        submit.to_csv(\n",
    "            r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\22Features.csv',\n",
    "            index=False)\n",
    "\n",
    "\n",
    "backward(train_x, train_y_, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络V2 ##\n",
    "\n",
    "- 交叉熵损失函数\n",
    "- 指数衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "STEPS = 25000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.005\n",
    "DECAY_STEPS = 222750\n",
    "DECAY_RATE = 0.96\n",
    "\n",
    "\n",
    "def get_weight(shape):\n",
    "    w = tf.Variable(tf.random_normal(shape))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.random_normal(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    w = get_weight([22, 1])\n",
    "    b = get_bias([1])\n",
    "    y = tf.matmul(x, w) + b\n",
    "    pred = tf.cast(tf.sigmoid(y) > 0.5, tf.float32)\n",
    "    return y, pred\n",
    "\n",
    "\n",
    "def backward(train_x, train_y_, test_x):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 22])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    y, pred = forward(x)\n",
    "\n",
    "    # 定义损失函数loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decayed_learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE)\n",
    "\n",
    "    # 定义反向传播方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        decayed_learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            index = np.random.permutation(len(train_y_))\n",
    "            train_x = train_x.take(index)\n",
    "            train_y_ = train_y_.take(index)\n",
    "            for j in range(len(train_y_) // 100 + 1):\n",
    "                start = j * BATCH_SIZE\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "            if i % 1000 == 0:\n",
    "                train_loss_temp = sess.run(\n",
    "                    loss,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_loss.append(train_loss_temp)\n",
    "                train_acc_temp = sess.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_acc.append(train_acc_temp)\n",
    "                print(train_loss_temp, ' ', train_acc_temp)\n",
    "                print('global_step:',\n",
    "                      sess.run(global_step), 'decayed_learning_rate:',\n",
    "                      sess.run(decayed_learning_rate))\n",
    "        # 使用训练好的模型，喂入测试集数据\n",
    "        result = sess.run(pred, feed_dict={x: test_x})\n",
    "        # print('----------------')\n",
    "        # print('Prediction:')\n",
    "        # print(result)\n",
    "        submit = test[['PassengerId']]\n",
    "        submit.insert(1, 'Survived', result)\n",
    "        submit['Survived'] = submit['Survived'].astype(np.int32)\n",
    "        submit.to_csv(\n",
    "            r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\22Features.csv',\n",
    "            index=False)\n",
    "\n",
    "\n",
    "backward(train_x, train_y_, test_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
