{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "train = pd.read_csv(\n",
    "    r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\train.csv')\n",
    "test = pd.read_csv(\n",
    "    r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\test.csv')\n",
    "full_data = [train, test]\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pclass ##\n",
    "\n",
    "票类：经济地位的象征\n",
    "\n",
    "序号 | 票类\n",
    "---- | ----\n",
    "1 | 头等舱\n",
    "2 | 中等舱\n",
    "3 | 末等舱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name   Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris  male  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1  \n"
     ]
    }
   ],
   "source": [
    "# One-hot编码\n",
    "# train\n",
    "train['P1'] = np.array(train['Pclass'] == 1).astype(np.int32)\n",
    "train['P2'] = np.array(train['Pclass'] == 2).astype(np.int32)\n",
    "train['P3'] = np.array(train['Pclass'] == 3).astype(np.int32)\n",
    "# test\n",
    "test['P1'] = np.array(test['Pclass'] == 1).astype(np.int32)\n",
    "test['P2'] = np.array(test['Pclass'] == 2).astype(np.int32)\n",
    "test['P3'] = np.array(test['Pclass'] == 3).astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sex ##\n",
    "\n",
    "性别：男or女\n",
    "\n",
    "Sex | label\n",
    "---- | ----\n",
    "male | 1\n",
    "female | 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1  \n"
     ]
    }
   ],
   "source": [
    "# 把male/female转换成1/0\n",
    "train['Sex'] = [1 if i == 'male' else 0 for i in train.Sex]\n",
    "test['Sex'] = [1 if i == 'male' else 0 for i in test.Sex]\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SibSp and Parch ##\n",
    "\n",
    "- SibSp\n",
    "\n",
    "the number of siblings/spouse：兄弟姐妹/配偶人数\n",
    "\n",
    "- Parch\n",
    "\n",
    "the number of children/parents：子女/父母人数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2  \n"
     ]
    }
   ],
   "source": [
    "# 'FamilySize'：家庭成员人数\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  IsAlone  \n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2        0  \n"
     ]
    }
   ],
   "source": [
    "# 'IsAlone'：是否只身一人\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embarked ##\n",
    "\n",
    "登船港口，有缺失值，先进行缺失值处理\n",
    "\n",
    "C = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare Cabin Embarked  P1  P2  P3  FamilySize  IsAlone  E1  \\\n",
      "0      0  A/5 21171  7.25   NaN        S   0   0   1           2        0   1   \n",
      "\n",
      "   E2  E3  \n",
      "0   0   0  \n"
     ]
    }
   ],
   "source": [
    "# 缺失值处理\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# One-hot编码\n",
    "# train\n",
    "train['E1'] = np.array(train['Embarked'] == 'S').astype(np.int32)\n",
    "train['E2'] = np.array(train['Embarked'] == 'C').astype(np.int32)\n",
    "train['E3'] = np.array(train['Embarked'] == 'Q').astype(np.int32)\n",
    "# test\n",
    "test['E1'] = np.array(test['Embarked'] == 'S').astype(np.int32)\n",
    "test['E2'] = np.array(test['Embarked'] == 'C').astype(np.int32)\n",
    "test['E3'] = np.array(test['Embarked'] == 'Q').astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fare ##\n",
    "\n",
    "乘客票价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex   Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1  22.0      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... FamilySize IsAlone  E1  E2  E3  CategoricalFare  \\\n",
      "0      0  A/5 21171  7.25 ...          2       0   1   0   0                1   \n",
      "\n",
      "   F1  F2  F3  F4  \n",
      "0   1   0   0   0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "train['CategoricalFare'].cat.categories = [1, 2, 3, 4]\n",
    "# one-hot编码\n",
    "train['F1'] = np.array(train['CategoricalFare'] == 1).astype(np.int32)\n",
    "train['F2'] = np.array(train['CategoricalFare'] == 2).astype(np.int32)\n",
    "train['F3'] = np.array(train['CategoricalFare'] == 3).astype(np.int32)\n",
    "train['F4'] = np.array(train['CategoricalFare'] == 4).astype(np.int32)\n",
    "\n",
    "# test\n",
    "test['CategoricalFare'] = pd.qcut(test['Fare'], 4)\n",
    "test['CategoricalFare'].cat.categories = [1, 2, 3, 4]\n",
    "# one-hot编码\n",
    "test['F1'] = np.array(test['CategoricalFare'] == 1).astype(np.int32)\n",
    "test['F2'] = np.array(test['CategoricalFare'] == 2).astype(np.int32)\n",
    "test['F3'] = np.array(test['CategoricalFare'] == 3).astype(np.int32)\n",
    "test['F4'] = np.array(test['CategoricalFare'] == 4).astype(np.int32)\n",
    "\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Age ##\n",
    "\n",
    "缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... FamilySize IsAlone  E1  E2  E3  CategoricalFare  \\\n",
      "0      0  A/5 21171  7.25 ...          2       0   1   0   0                1   \n",
      "\n",
      "   F1  F2  F3  F4  \n",
      "0   1   0   0   0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(\n",
    "        age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... F1 F2  F3  F4  CategoricalAge  A1  A2  A3  A4  \\\n",
      "0      0  A/5 21171  7.25 ...  1  0   0   0               2   0   1   0   0   \n",
      "\n",
      "   A5  \n",
      "0   0  \n",
      "\n",
      "[1 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['CategoricalAge'] = pd.qcut(train['Age'], 5)\n",
    "train['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]\n",
    "train['A1'] = np.array(train['CategoricalAge'] == 1).astype(np.int32)\n",
    "train['A2'] = np.array(train['CategoricalAge'] == 2).astype(np.int32)\n",
    "train['A3'] = np.array(train['CategoricalAge'] == 3).astype(np.int32)\n",
    "train['A4'] = np.array(train['CategoricalAge'] == 4).astype(np.int32)\n",
    "train['A5'] = np.array(train['CategoricalAge'] == 5).astype(np.int32)\n",
    "# test\n",
    "test['CategoricalAge'] = pd.qcut(test['Age'], 5)\n",
    "test['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]\n",
    "test['A1'] = np.array(test['CategoricalAge'] == 1).astype(np.int32)\n",
    "test['A2'] = np.array(test['CategoricalAge'] == 2).astype(np.int32)\n",
    "test['A3'] = np.array(test['CategoricalAge'] == 3).astype(np.int32)\n",
    "test['A4'] = np.array(test['CategoricalAge'] == 4).astype(np.int32)\n",
    "test['A5'] = np.array(test['CategoricalAge'] == 5).astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Name ##\n",
    "\n",
    "新增一列特征'Title'：头衔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex         0    1\n",
      "Title             \n",
      "Capt        0    1\n",
      "Col         0    2\n",
      "Countess    1    0\n",
      "Don         0    1\n",
      "Dr          1    6\n",
      "Jonkheer    0    1\n",
      "Lady        1    0\n",
      "Major       0    2\n",
      "Master      0   40\n",
      "Miss      182    0\n",
      "Mlle        2    0\n",
      "Mme         1    0\n",
      "Mr          0  517\n",
      "Mrs       125    0\n",
      "Ms          1    0\n",
      "Rev         0    6\n",
      "Sir         0    1\n"
     ]
    }
   ],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "  'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "print(train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass                     Name  Sex  Age  SibSp  \\\n",
      "0            1         0       3  Braund, Mr. Owen Harris    1   22      1   \n",
      "\n",
      "   Parch     Ticket  Fare ... A2 A3  A4  A5  Title  T1  T2  T3  T4  T5  \n",
      "0      0  A/5 21171  7.25 ...  1  0   0   0     Mr   0   0   1   0   0  \n",
      "\n",
      "[1 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train['T1'] = np.array(train['Title'] == 'Master').astype(np.int32)\n",
    "train['T2'] = np.array(train['Title'] == 'Miss').astype(np.int32)\n",
    "train['T3'] = np.array(train['Title'] == 'Mr').astype(np.int32)\n",
    "train['T4'] = np.array(train['Title'] == 'Mrs').astype(np.int32)\n",
    "train['T5'] = np.array(train['Title'] == 'Rare').astype(np.int32)\n",
    "# test\n",
    "test['T1'] = np.array(test['Title'] == 'Master').astype(np.int32)\n",
    "test['T2'] = np.array(test['Title'] == 'Miss').astype(np.int32)\n",
    "test['T3'] = np.array(test['Title'] == 'Mr').astype(np.int32)\n",
    "test['T4'] = np.array(test['Title'] == 'Mrs').astype(np.int32)\n",
    "test['T5'] = np.array(test['Title'] == 'Rare').astype(np.int32)\n",
    "print(train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗 #\n",
    "\n",
    "获得训练神经网络的数据：**train_x，train_y_**\n",
    "\n",
    "以及预测样本：**test_x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'P1', 'P2', 'P3',\n",
       "       'FamilySize', 'IsAlone', 'E1', 'E2', 'E3', 'CategoricalFare', 'F1',\n",
       "       'F2', 'F3', 'F4', 'CategoricalAge', 'A1', 'A2', 'A3', 'A4', 'A5',\n",
       "       'Title', 'T1', 'T2', 'T3', 'T4', 'T5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   P1  P2  P3  Sex  IsAlone  E1  E2  E3  F1  F2 ...  A1  A2  A3  A4  A5  T1  \\\n",
      "0   0   0   1    1        0   1   0   0   1   0 ...   0   1   0   0   0   0   \n",
      "\n",
      "   T2  T3  T4  T5  \n",
      "0   0   1   0   0  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "train_x = train[[\n",
    "    'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3',\n",
    "    'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5'\n",
    "]]\n",
    "print(train_x.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived\n",
      "0         0\n"
     ]
    }
   ],
   "source": [
    "train_y_ = train[['Survived']]\n",
    "print(train_y_.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   P1  P2  P3  Sex  IsAlone  E1  E2  E3  F1  F2 ...  A1  A2  A3  A4  A5  T1  \\\n",
      "0   0   0   1    1        1   0   0   1   1   0 ...   0   0   0   1   0   0   \n",
      "\n",
      "   T2  T3  T4  T5  \n",
      "0   0   1   0   0  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "test_x = test[[\n",
    "    'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3',\n",
    "    'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5'\n",
    "]]\n",
    "print(test_x.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建神经网络 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络V3 ##\n",
    "\n",
    "- 交叉熵损失函数\n",
    "- 指数衰减学习率\n",
    "    - LEARNING_RATE = 0.001\n",
    "    - DECAY_STEPS = 222750\n",
    "    - DECAY_RATE = 0.96\n",
    "- 单个隐藏层，有44个神经元\n",
    "- 正则化\n",
    "    - REGULARIZER = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.682495   0.7692308\n",
      "global_step: 9      decayed_learning_rate: 0.0009999984\n",
      "8.393337   0.7582418\n",
      "global_step: 9009      decayed_learning_rate: 0.0009983503\n",
      "1.9252509   0.7582418\n",
      "global_step: 18009      decayed_learning_rate: 0.0009967051\n",
      "0.881907   0.6593407\n",
      "global_step: 27009      decayed_learning_rate: 0.0009950625\n",
      "0.71087724   0.6043956\n",
      "global_step: 36009      decayed_learning_rate: 0.0009934226\n",
      "0.6669779   0.63736266\n",
      "global_step: 45009      decayed_learning_rate: 0.0009917854\n",
      "0.6814368   0.5934066\n",
      "global_step: 54009      decayed_learning_rate: 0.000990151\n",
      "0.64777887   0.6593407\n",
      "global_step: 63009      decayed_learning_rate: 0.0009885192\n",
      "0.6677663   0.61538464\n",
      "global_step: 72009      decayed_learning_rate: 0.0009868902\n",
      "0.66182476   0.62637365\n",
      "global_step: 81009      decayed_learning_rate: 0.0009852637\n",
      "0.6769042   0.5934066\n",
      "global_step: 90009      decayed_learning_rate: 0.00098364\n",
      "0.68714106   0.5714286\n",
      "global_step: 99009      decayed_learning_rate: 0.0009820189\n",
      "0.6715865   0.6043956\n",
      "global_step: 108009      decayed_learning_rate: 0.0009804006\n",
      "0.70252   0.53846157\n",
      "global_step: 117009      decayed_learning_rate: 0.0009787848\n",
      "0.6509124   0.64835167\n",
      "global_step: 126009      decayed_learning_rate: 0.0009771718\n",
      "0.68183976   0.5824176\n",
      "global_step: 135009      decayed_learning_rate: 0.0009755614\n",
      "0.6560426   0.63736266\n",
      "global_step: 144009      decayed_learning_rate: 0.00097395363\n",
      "0.66120774   0.62637365\n",
      "global_step: 153009      decayed_learning_rate: 0.00097234856\n",
      "0.6766762   0.5934066\n",
      "global_step: 162009      decayed_learning_rate: 0.0009707461\n",
      "0.67665446   0.5934066\n",
      "global_step: 171009      decayed_learning_rate: 0.0009691463\n",
      "0.68184376   0.5824176\n",
      "global_step: 180009      decayed_learning_rate: 0.00096754916\n",
      "0.7076919   0.52747256\n",
      "global_step: 189009      decayed_learning_rate: 0.00096595456\n",
      "0.6663159   0.61538464\n",
      "global_step: 198009      decayed_learning_rate: 0.00096436264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-421efab1ee9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-421efab1ee9f>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(train_x, train_y_, test_x)\u001b[0m\n\u001b[0;32m     77\u001b[0m                     feed_dict={\n\u001b[0;32m     78\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                         \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m                     })\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEPS = 25000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_STEPS = 222750\n",
    "DECAY_RATE = 0.96\n",
    "REGULARIZER = 0.1\n",
    "\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses',\n",
    "                         tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.random_normal(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    w1 = get_weight([22, 44], REGULARIZER)\n",
    "    y1 = tf.sigmoid(tf.matmul(x, w1))\n",
    "\n",
    "    w2 = get_weight([44, 1], REGULARIZER)\n",
    "    b = get_bias([1])\n",
    "    y = tf.matmul(y1, w2) + b\n",
    "    pred = tf.cast(tf.sigmoid(y) > 0.5, tf.float32)\n",
    "    return y, pred\n",
    "\n",
    "\n",
    "def backward(train_x, train_y_, test_x):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 22])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    y, pred = forward(x)\n",
    "\n",
    "    # 定义损失函数cross_entropy_loss\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    tf.add_to_collection('losses', cross_entropy_loss)\n",
    "    \n",
    "    # 定义总的损失函数\n",
    "    loss = tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decayed_learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE)\n",
    "\n",
    "    # 定义反向传播方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        decayed_learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            index = np.random.permutation(len(train_y_))\n",
    "            train_x = train_x.take(index)\n",
    "            train_y_ = train_y_.take(index)\n",
    "            for j in range(len(train_y_) // 100 + 1):\n",
    "                start = j * BATCH_SIZE\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "            if i % 1000 == 0:\n",
    "                train_loss_temp = sess.run(\n",
    "                    loss,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_loss.append(train_loss_temp)\n",
    "                train_acc_temp = sess.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_acc.append(train_acc_temp)\n",
    "                print(train_loss_temp, ' ', train_acc_temp)\n",
    "                print('global_step:', sess.run(global_step),\n",
    "                      '    ', 'decayed_learning_rate:',\n",
    "                      sess.run(decayed_learning_rate))\n",
    "        # 使用训练好的模型，喂入测试集数据\n",
    "        result = sess.run(pred, feed_dict={x: test_x})\n",
    "        # print('----------------')\n",
    "        # print('Prediction:')\n",
    "        # print(result)\n",
    "        submit = test[['PassengerId']]\n",
    "        submit.insert(1, 'Survived', result)\n",
    "        submit['Survived'] = submit['Survived'].astype(np.int32)\n",
    "        submit.to_csv(\n",
    "            r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\22Features.csv',\n",
    "            index=False)\n",
    "\n",
    "\n",
    "backward(train_x, train_y_, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络V1 ##\n",
    "\n",
    "- 无隐藏层\n",
    "- 交叉熵损失函数\n",
    "- 指数衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bbd267750b85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mSTEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "STEPS = 25000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.005\n",
    "DECAY_STEPS = 222750\n",
    "DECAY_RATE = 0.96\n",
    "\n",
    "\n",
    "def get_weight(shape):\n",
    "    w = tf.Variable(tf.random_normal(shape))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.random_normal(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    w = get_weight([22, 1])\n",
    "    b = get_bias([1])\n",
    "    y = tf.matmul(x, w) + b\n",
    "    pred = tf.cast(tf.sigmoid(y) > 0.5, tf.float32)\n",
    "    return y, pred\n",
    "\n",
    "\n",
    "def backward(train_x, train_y_, test_x):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 22])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    y, pred = forward(x)\n",
    "\n",
    "    # 定义损失函数loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decayed_learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE)\n",
    "\n",
    "    # 定义反向传播方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        decayed_learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            index = np.random.permutation(len(train_y_))\n",
    "            train_x = train_x.take(index)\n",
    "            train_y_ = train_y_.take(index)\n",
    "            for j in range(len(train_y_) // 100 + 1):\n",
    "                start = j * BATCH_SIZE\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "            if i % 1000 == 0:\n",
    "                train_loss_temp = sess.run(\n",
    "                    loss,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_loss.append(train_loss_temp)\n",
    "                train_acc_temp = sess.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_acc.append(train_acc_temp)\n",
    "                print(train_loss_temp, ' ', train_acc_temp)\n",
    "                print('global_step:',\n",
    "                      sess.run(global_step), 'decayed_learning_rate:',\n",
    "                      sess.run(decayed_learning_rate))\n",
    "        # 使用训练好的模型，喂入测试集数据\n",
    "        result = sess.run(pred, feed_dict={x: test_x})\n",
    "        # print('----------------')\n",
    "        # print('Prediction:')\n",
    "        # print(result)\n",
    "        submit = test[['PassengerId']]\n",
    "        submit.insert(1, 'Survived', result)\n",
    "        submit['Survived'] = submit['Survived'].astype(np.int32)\n",
    "        submit.to_csv(\n",
    "            r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\22Features.csv',\n",
    "            index=False)\n",
    "\n",
    "\n",
    "backward(train_x, train_y_, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络V2 ##\n",
    "\n",
    "- 交叉熵损失函数\n",
    "- 指数衰减学习率\n",
    "    - LEARNING_RATE = 0.001\n",
    "    - DECAY_STEPS = 222750\n",
    "    - DECAY_RATE = 0.96\n",
    "- 隐藏层有44个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.904674   0.43956044\n",
      "global_step: 9      decayed_learning_rate: 0.0009999984\n",
      "0.50262743   0.7912088\n",
      "global_step: 9009      decayed_learning_rate: 0.0009983503\n",
      "0.39607292   0.82417583\n",
      "global_step: 18009      decayed_learning_rate: 0.0009967051\n",
      "0.56316733   0.7582418\n",
      "global_step: 27009      decayed_learning_rate: 0.0009950625\n",
      "0.42858267   0.82417583\n",
      "global_step: 36009      decayed_learning_rate: 0.0009934226\n",
      "0.39582002   0.82417583\n",
      "global_step: 45009      decayed_learning_rate: 0.0009917854\n",
      "0.47426587   0.7692308\n",
      "global_step: 54009      decayed_learning_rate: 0.000990151\n",
      "0.32033125   0.84615386\n",
      "global_step: 63009      decayed_learning_rate: 0.0009885192\n",
      "0.43546927   0.85714287\n",
      "global_step: 72009      decayed_learning_rate: 0.0009868902\n",
      "0.3717739   0.84615386\n",
      "global_step: 81009      decayed_learning_rate: 0.0009852637\n",
      "0.42223954   0.82417583\n",
      "global_step: 90009      decayed_learning_rate: 0.00098364\n",
      "0.41715768   0.84615386\n",
      "global_step: 99009      decayed_learning_rate: 0.0009820189\n",
      "0.42711145   0.82417583\n",
      "global_step: 108009      decayed_learning_rate: 0.0009804006\n",
      "0.34866366   0.8681319\n",
      "global_step: 117009      decayed_learning_rate: 0.0009787848\n",
      "0.48564047   0.8021978\n",
      "global_step: 126009      decayed_learning_rate: 0.0009771718\n",
      "0.36750388   0.85714287\n",
      "global_step: 135009      decayed_learning_rate: 0.0009755614\n",
      "0.38596904   0.9010989\n",
      "global_step: 144009      decayed_learning_rate: 0.00097395363\n",
      "0.45860165   0.7802198\n",
      "global_step: 153009      decayed_learning_rate: 0.00097234856\n",
      "0.4228753   0.83516484\n",
      "global_step: 162009      decayed_learning_rate: 0.0009707461\n",
      "0.41535723   0.82417583\n",
      "global_step: 171009      decayed_learning_rate: 0.0009691463\n",
      "0.37949723   0.84615386\n",
      "global_step: 180009      decayed_learning_rate: 0.00096754916\n",
      "0.3682254   0.84615386\n",
      "global_step: 189009      decayed_learning_rate: 0.00096595456\n",
      "0.39057797   0.83516484\n",
      "global_step: 198009      decayed_learning_rate: 0.00096436264\n",
      "0.39697516   0.82417583\n",
      "global_step: 207009      decayed_learning_rate: 0.0009627734\n",
      "0.45209754   0.82417583\n",
      "global_step: 216009      decayed_learning_rate: 0.0009611868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "STEPS = 25000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_STEPS = 222750\n",
    "DECAY_RATE = 0.96\n",
    "\n",
    "\n",
    "def get_weight(shape):\n",
    "    w = tf.Variable(tf.random_normal(shape))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.random_normal(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    w1 = get_weight([22, 44])\n",
    "    y1 = tf.matmul(x, w1)\n",
    "    \n",
    "    w2 = get_weight([44, 1])\n",
    "    b = get_bias([1])\n",
    "    y = tf.matmul(y1, w2) + b\n",
    "    pred = tf.cast(tf.sigmoid(y) > 0.5, tf.float32)\n",
    "    return y, pred\n",
    "\n",
    "\n",
    "def backward(train_x, train_y_, test_x):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 22])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    y, pred = forward(x)\n",
    "\n",
    "    # 定义损失函数loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decayed_learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE, global_step, DECAY_STEPS, DECAY_RATE)\n",
    "\n",
    "    # 定义反向传播方法\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        decayed_learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            index = np.random.permutation(len(train_y_))\n",
    "            train_x = train_x.take(index)\n",
    "            train_y_ = train_y_.take(index)\n",
    "            for j in range(len(train_y_) // 100 + 1):\n",
    "                start = j * BATCH_SIZE\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(\n",
    "                    train_step,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "            if i % 1000 == 0:\n",
    "                train_loss_temp = sess.run(\n",
    "                    loss,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_loss.append(train_loss_temp)\n",
    "                train_acc_temp = sess.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        x: train_x[start:end],\n",
    "                        y_: train_y_[start:end]\n",
    "                    })\n",
    "                train_acc.append(train_acc_temp)\n",
    "                print(train_loss_temp, ' ', train_acc_temp)\n",
    "                print('global_step:',\n",
    "                      sess.run(global_step), '    ', 'decayed_learning_rate:',\n",
    "                      sess.run(decayed_learning_rate))\n",
    "        # 使用训练好的模型，喂入测试集数据\n",
    "        result = sess.run(pred, feed_dict={x: test_x})\n",
    "        # print('----------------')\n",
    "        # print('Prediction:')\n",
    "        # print(result)\n",
    "        submit = test[['PassengerId']]\n",
    "        submit.insert(1, 'Survived', result)\n",
    "        submit['Survived'] = submit['Survived'].astype(np.int32)\n",
    "        submit.to_csv(\n",
    "            r'E:\\Mirror\\GitHub\\Predict-survival-on-the-Titanic\\data\\22Features.csv',\n",
    "            index=False)\n",
    "\n",
    "\n",
    "backward(train_x, train_y_, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
